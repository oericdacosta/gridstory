#!/usr/bin/env python3
"""
CLI para anÃ¡lise de Machine Learning com tracking MLFlow.

Este script executa anÃ¡lises de ML (clustering + anomaly detection) em dados
de corridas jÃ¡ processados, com tracking completo de mÃ©tricas via MLFlow.

Usage:
    # AnÃ¡lise completa com MLFlow tracking
    uv run python cli/ml_analysis.py --year 2025 --round 1 --mlflow

    # AnÃ¡lise apenas de clustering
    uv run python cli/ml_analysis.py --year 2025 --round 1 --clustering --mlflow

    # AnÃ¡lise apenas de anomaly detection
    uv run python cli/ml_analysis.py --year 2025 --round 1 --anomaly --mlflow

    # AnÃ¡lise de piloto especÃ­fico
    uv run python cli/ml_analysis.py --year 2025 --round 1 --driver VER --mlflow

    # Comparar runs anteriores
    uv run python cli/ml_analysis.py --compare --experiment "F1_2025_Round_01"
"""

import argparse
import sys
from pathlib import Path

import pandas as pd

# Adicionar raiz do projeto ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.ml import (
    run_race_analysis,
    setup_mlflow,
    compare_runs,
    get_best_run,
)


def parse_args():
    """Parse argumentos de linha de comando."""
    parser = argparse.ArgumentParser(
        description="AnÃ¡lise de ML com tracking MLFlow",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # AnÃ¡lise completa com MLFlow
  uv run python cli/ml_analysis.py --year 2025 --round 1 --mlflow

  # Apenas clustering
  uv run python cli/ml_analysis.py --year 2025 --round 1 --clustering --mlflow

  # Piloto especÃ­fico
  uv run python cli/ml_analysis.py --year 2025 --round 1 --driver VER --mlflow

  # Comparar runs
  uv run python cli/ml_analysis.py --compare --experiment "F1_2025_Round_01"
        """
    )

    # Grupo: AnÃ¡lise de corrida
    race_group = parser.add_argument_group('AnÃ¡lise de corrida')
    race_group.add_argument(
        '--year',
        type=int,
        help='Ano da corrida (ex: 2025)'
    )
    race_group.add_argument(
        '--round',
        type=int,
        help='NÃºmero da rodada (ex: 1 para primeira corrida)'
    )
    race_group.add_argument(
        '--driver',
        type=str,
        help='CÃ³digo do piloto para anÃ¡lise especÃ­fica (ex: VER, HAM, LEC)'
    )

    # Grupo: Tipo de anÃ¡lise
    analysis_group = parser.add_argument_group('Tipo de anÃ¡lise')
    analysis_group.add_argument(
        '--clustering',
        action='store_true',
        help='Executar apenas clustering (K-Means)'
    )
    analysis_group.add_argument(
        '--anomaly',
        action='store_true',
        help='Executar apenas detecÃ§Ã£o de anomalias (Isolation Forest)'
    )

    # Grupo: MLFlow
    mlflow_group = parser.add_argument_group('MLFlow')
    mlflow_group.add_argument(
        '--mlflow',
        action='store_true',
        help='Habilitar tracking com MLFlow'
    )
    mlflow_group.add_argument(
        '--experiment',
        type=str,
        help='Nome do experimento MLFlow (padrÃ£o: F1_YEAR_Round_XX)'
    )
    mlflow_group.add_argument(
        '--run-name',
        type=str,
        help='Nome do run MLFlow (padrÃ£o: auto-gerado)'
    )

    # Grupo: ComparaÃ§Ã£o de runs
    compare_group = parser.add_argument_group('ComparaÃ§Ã£o de runs')
    compare_group.add_argument(
        '--compare',
        action='store_true',
        help='Comparar runs anteriores'
    )
    compare_group.add_argument(
        '--best',
        action='store_true',
        help='Mostrar melhor run baseado em silhouette score'
    )
    compare_group.add_argument(
        '--max-runs',
        type=int,
        default=10,
        help='NÃºmero mÃ¡ximo de runs para comparar (padrÃ£o: 10)'
    )

    # Grupo: Output
    output_group = parser.add_argument_group('Output')
    output_group.add_argument(
        '--save',
        action='store_true',
        help='Salvar resultados em data/ml/'
    )
    output_group.add_argument(
        '--show-metrics',
        action='store_true',
        help='Mostrar mÃ©tricas detalhadas no terminal'
    )

    return parser.parse_args()


def load_race_data(year: int, round_number: int) -> pd.DataFrame | None:
    """
    Carrega dados processados de uma corrida.

    Args:
        year: Ano da corrida
        round_number: NÃºmero da rodada

    Returns:
        DataFrame com dados de voltas ou None se nÃ£o encontrado
    """
    processed_dir = Path("data/processed/races")

    # Tentar carregar laps processados
    laps_file = processed_dir / f"{year}/round_{round_number:02d}/laps_processed.parquet"

    if not laps_file.exists():
        print(f"âŒ Arquivo nÃ£o encontrado: {laps_file}")
        print(f"   Execute primeiro: uv run python cli/pipeline.py {year} {round_number}")
        return None

    print(f"ğŸ“‚ Carregando dados de: {laps_file}")
    laps_df = pd.read_parquet(laps_file)

    # Adicionar Year e Round se nÃ£o existirem
    if 'Year' not in laps_df.columns:
        laps_df['Year'] = year
    if 'Round' not in laps_df.columns:
        laps_df['Round'] = round_number

    return laps_df


def save_results(results: dict, year: int, round_number: int, driver: str | None = None):
    """
    Salva resultados da anÃ¡lise.

    Args:
        results: DicionÃ¡rio com resultados da anÃ¡lise
        year: Ano da corrida
        round_number: NÃºmero da rodada
        driver: CÃ³digo do piloto (opcional)
    """
    ml_dir = Path("data/ml/races")

    # Criar diretÃ³rio de saÃ­da
    output_dir = ml_dir / f"{year}/round_{round_number:02d}"
    output_dir.mkdir(parents=True, exist_ok=True)

    # Sufixo para piloto especÃ­fico
    suffix = f"_{driver}" if driver else ""

    # Salvar DataFrames
    if 'laps_clustered' in results:
        output_file = output_dir / f"laps_clustered{suffix}.parquet"
        results['laps_clustered'].to_parquet(output_file, index=False)
        print(f"âœ… Salvo: {output_file}")

    if 'laps_anomalies' in results:
        output_file = output_dir / f"laps_anomalies{suffix}.parquet"
        results['laps_anomalies'].to_parquet(output_file, index=False)
        print(f"âœ… Salvo: {output_file}")

    if 'cluster_statistics' in results:
        output_file = output_dir / f"cluster_statistics{suffix}.parquet"
        results['cluster_statistics'].to_parquet(output_file, index=False)
        print(f"âœ… Salvo: {output_file}")

    if 'summary' in results:
        output_file = output_dir / f"analysis_summary{suffix}.parquet"
        results['summary'].to_parquet(output_file, index=False)
        print(f"âœ… Salvo: {output_file}")


def print_metrics(results: dict):
    """
    Imprime mÃ©tricas de forma formatada.

    Args:
        results: DicionÃ¡rio com resultados da anÃ¡lise
    """
    print("\n" + "="*60)
    print("ğŸ“Š MÃ‰TRICAS DA ANÃLISE")
    print("="*60)

    # Summary
    if 'summary' in results:
        print("\nğŸ“‹ SumÃ¡rio Geral:")
        for col in results['summary'].columns:
            value = results['summary'][col].iloc[0]
            print(f"   {col}: {value}")

    # Clustering metrics
    if 'clustering_metrics' in results:
        print("\nğŸ¯ MÃ©tricas de Clustering:")
        metrics_df = results['clustering_metrics']
        for col in metrics_df.columns:
            value = metrics_df[col].iloc[0]
            if isinstance(value, float):
                print(f"   {col}: {value:.4f}")
            else:
                print(f"   {col}: {value}")

    # Anomaly metrics
    if 'anomaly_metrics' in results:
        print("\nğŸ” MÃ©tricas de DetecÃ§Ã£o de Anomalias:")
        metrics_df = results['anomaly_metrics']
        for col in metrics_df.columns:
            value = metrics_df[col].iloc[0]
            if isinstance(value, float):
                print(f"   {col}: {value:.4f}")
            else:
                print(f"   {col}: {value}")

    # Cluster statistics
    if 'cluster_statistics' in results:
        print("\nğŸ“ˆ EstatÃ­sticas por Cluster:")
        print(results['cluster_statistics'].to_string(index=False))

    print("\n" + "="*60)


def compare_experiment_runs(experiment_name: str, max_runs: int = 10):
    """
    Compara runs de um experimento.

    Args:
        experiment_name: Nome do experimento
        max_runs: NÃºmero mÃ¡ximo de runs para comparar
    """
    print(f"\nğŸ”¬ Comparando runs do experimento: {experiment_name}")
    print("="*60)

    comparison = compare_runs(
        experiment_name=experiment_name,
        metric_names=['silhouette_score', 'davies_bouldin_score', 'n_anomalies', 'anomaly_rate'],
        max_runs=max_runs
    )

    if comparison.empty:
        print("âŒ Nenhum run encontrado para este experimento.")
        return

    # Mostrar comparaÃ§Ã£o
    print("\nğŸ“Š ComparaÃ§Ã£o de Runs:")
    print(comparison.to_string(index=False))

    # Mostrar melhor run
    best = get_best_run(experiment_name, 'silhouette_score', ascending=False)
    if best:
        print(f"\nğŸ† Melhor Run (Silhouette Score):")
        print(f"   Run ID: {best['run_id']}")
        print(f"   Run Name: {best['run_name']}")
        print(f"   MÃ©tricas:")
        for metric, value in best['metrics'].items():
            if isinstance(value, float):
                print(f"      {metric}: {value:.4f}")
            else:
                print(f"      {metric}: {value}")


def main():
    """FunÃ§Ã£o principal."""
    args = parse_args()

    # Modo comparaÃ§Ã£o
    if args.compare:
        if not args.experiment:
            print("âŒ Erro: --experiment Ã© requerido para comparaÃ§Ã£o")
            sys.exit(1)

        compare_experiment_runs(args.experiment, args.max_runs)
        return

    # Modo anÃ¡lise
    if not args.year or not args.round:
        print("âŒ Erro: --year e --round sÃ£o requeridos")
        sys.exit(1)

    # Carregar dados
    laps_df = load_race_data(args.year, args.round)
    if laps_df is None:
        sys.exit(1)

    print(f"ğŸ“Š Total de voltas carregadas: {len(laps_df)}")
    if 'Driver' in laps_df.columns:
        print(f"ğŸ‘¥ Pilotos: {laps_df['Driver'].nunique()}")

    # Determinar tipo de anÃ¡lise
    if args.clustering and not args.anomaly:
        analysis_type = 'clustering'
    elif args.anomaly and not args.clustering:
        analysis_type = 'anomaly'
    else:
        analysis_type = 'all'

    print(f"ğŸ”¬ Tipo de anÃ¡lise: {analysis_type}")

    # Determinar nome do experimento MLFlow
    experiment_name = args.experiment
    if args.mlflow and experiment_name is None:
        experiment_name = f"F1_{args.year}_Round_{args.round:02d}"

    # Executar anÃ¡lise
    print("\nâš™ï¸  Executando anÃ¡lise de ML...")
    results = run_race_analysis(
        laps_df=laps_df,
        analysis_type=analysis_type,
        driver=args.driver,
        enable_mlflow=args.mlflow,
        experiment_name=experiment_name,
        run_name=args.run_name,
    )

    # Mostrar mÃ©tricas
    if args.show_metrics:
        print_metrics(results)
    else:
        # Mostrar sumÃ¡rio bÃ¡sico
        print("\nâœ… AnÃ¡lise concluÃ­da!")
        if 'summary' in results:
            print("\nğŸ“‹ SumÃ¡rio:")
            print(results['summary'].to_string(index=False))

    # MLFlow run ID
    if 'mlflow_run_id' in results:
        print(f"\nğŸ“Š MLFlow Run ID: {results['mlflow_run_id']}")
        print(f"   Experimento: {experiment_name}")
        print(f"\nğŸ’¡ Para visualizar no MLFlow UI:")
        print(f"   mlflow ui")
        print(f"   Depois acesse: http://localhost:5000")

    # Salvar resultados
    if args.save:
        print("\nğŸ’¾ Salvando resultados...")
        save_results(results, args.year, args.round, args.driver)

    print("\nâœ¨ ConcluÃ­do!")


if __name__ == "__main__":
    main()
